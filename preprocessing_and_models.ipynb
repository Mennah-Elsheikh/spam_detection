{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detection Pipeline\n",
    "\n",
    "Complete implementation with:\n",
    "1. Data Preprocessing\n",
    "2. Feature Extraction (Binary, Count, TF-IDF, Word2Vec)\n",
    "3. Model Training & Evaluation (Naive Bayes, SVM, Random Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to dataset\n",
    "path = r\"C:\\Users\\Sroor For Laptop\\.cache\\kagglehub\\datasets\\uciml\\sms-spam-collection-dataset\\versions\\1\"\n",
    "filepath = os.path.join(path, \"spam.csv\")\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(filepath, encoding='latin-1')\n",
    "\n",
    "# Rename columns\n",
    "df.rename(columns={'v1': 'label', 'v2': 'text'}, inplace=True)\n",
    "\n",
    "# Keep only relevant columns\n",
    "df = df[['label', 'text']]\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nClass distribution:\\n{df['label'].value_counts()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and preprocess text\"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"Preprocessing text...\")\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "df['clean_tokens'] = df['cleaned_text'].apply(word_tokenize)\n",
    "\n",
    "print(\"Preprocessing complete!\")\n",
    "df[['text', 'cleaned_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction\n",
    "\n",
    "### a) Binary Encoding (Sentence-level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting Binary Encoding features...\")\n",
    "binary_vectorizer = CountVectorizer(binary=True)\n",
    "X_binary = binary_vectorizer.fit_transform(df['cleaned_text'])\n",
    "print(f\"Binary Encoding Shape: {X_binary.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Count Vectorization (Sentence-level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting Count Vectorization features...\")\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_count = count_vectorizer.fit_transform(df['cleaned_text'])\n",
    "print(f\"Count Vectorization Shape: {X_count.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) TF-IDF (Sentence-level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting TF-IDF features...\")\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_text'])\n",
    "print(f\"TF-IDF Shape: {X_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Word2Vec (Word-level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Word2Vec model on words...\")\n",
    "\n",
    "# Prepare tokenized sentences\n",
    "tokenized_sentences = df['clean_tokens'].tolist()\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to average word vectors for a document\n",
    "def document_vector(doc):\n",
    "    # Remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in w2v_model.wv.index_to_key]\n",
    "    if not doc:\n",
    "        return np.zeros(100)\n",
    "    return np.mean(w2v_model.wv[doc], axis=0)\n",
    "\n",
    "X_word2vec = np.array([document_vector(doc) for doc in tokenized_sentences])\n",
    "print(f\"Word2Vec Shape: {X_word2vec.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to numeric\n",
    "y = df['label'].map({'ham': 0, 'spam': 1})\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"Label distribution:\\n{y.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training & Evaluation\n",
    "\n",
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"Train and evaluate a single model\"\"\"\n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, pos_label=1, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, pos_label=1, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, pos_label=1, zero_division=0)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n--- {model_name} ---\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'predictions': y_pred,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "\n",
    "def evaluate_feature_set(X, y, feature_name, use_naive_bayes=True):\n",
    "    \"\"\"Evaluate all models on a feature set\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Evaluating Models on {feature_name} Features\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Naive Bayes (only for non-negative features)\n",
    "    if use_naive_bayes:\n",
    "        nb_model = MultinomialNB()\n",
    "        results['Naive Bayes'] = train_evaluate_model(nb_model, X_train, X_test, y_train, y_test, 'Naive Bayes')\n",
    "    \n",
    "    # SVM\n",
    "    svm_model = SVC(kernel='linear', probability=True)\n",
    "    results['SVM'] = train_evaluate_model(svm_model, X_train, X_test, y_train, y_test, 'SVM')\n",
    "    \n",
    "    # Random Forest\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    results['Random Forest'] = train_evaluate_model(rf_model, X_train, X_test, y_train, y_test, 'Random Forest')\n",
    "    \n",
    "    return results, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Binary Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_results, X_test_binary, y_test_binary = evaluate_feature_set(X_binary, y, \"Binary Encoding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_results, X_test_count, y_test_count = evaluate_feature_set(X_count, y, \"Count Vectorization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_results, X_test_tfidf, y_test_tfidf = evaluate_feature_set(X_tfidf, y, \"TF-IDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Word2Vec (Word-level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec has negative values, so Naive Bayes won't work\n",
    "w2v_results, X_test_w2v, y_test_w2v = evaluate_feature_set(X_word2vec, y, \"Word2Vec (Word-level)\", use_naive_bayes=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Find Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results\n",
    "all_results = {\n",
    "    'Binary Encoding': binary_results,\n",
    "    'Count Vectorization': count_results,\n",
    "    'TF-IDF': tfidf_results,\n",
    "    'Word2Vec': w2v_results\n",
    "}\n",
    "\n",
    "# Find best model\n",
    "best_f1 = 0\n",
    "best_model_info = None\n",
    "\n",
    "for feature_name, models in all_results.items():\n",
    "    for model_name, result in models.items():\n",
    "        f1 = result['f1_score']\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_model_info = (feature_name, model_name, result)\n",
    "\n",
    "if best_model_info:\n",
    "    feature_name, model_name, result = best_model_info\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"BEST MODEL: {model_name} with {feature_name} features\")\n",
    "    print(f\"F1 Score: {best_f1:.4f}\")\n",
    "    print(f\"Accuracy: {result['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {result['precision']:.4f}\")\n",
    "    print(f\"Recall: {result['recall']:.4f}\")\n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "# Save best model\n",
    "if best_model_info:\n",
    "    feature_name, model_name, result = best_model_info\n",
    "    model_path = os.path.join('models', 'best_spam_model.pkl')\n",
    "    joblib.dump(result['model'], model_path)\n",
    "    print(f\"\\nBest model saved to {model_path}\")\n",
    "    \n",
    "    # Save appropriate vectorizer\n",
    "    if 'Binary' in feature_name:\n",
    "        vectorizer_path = os.path.join('models', 'binary_vectorizer.pkl')\n",
    "        joblib.dump(binary_vectorizer, vectorizer_path)\n",
    "    elif 'Count' in feature_name:\n",
    "        vectorizer_path = os.path.join('models', 'count_vectorizer.pkl')\n",
    "        joblib.dump(count_vectorizer, vectorizer_path)\n",
    "    elif 'TF-IDF' in feature_name:\n",
    "        vectorizer_path = os.path.join('models', 'tfidf_vectorizer.pkl')\n",
    "        joblib.dump(tfidf_vectorizer, vectorizer_path)\n",
    "    \n",
    "    print(f\"Vectorizer saved to {vectorizer_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Confusion Matrix for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_model_info:\n",
    "    feature_name, model_name, result = best_model_info\n",
    "    \n",
    "    # Get appropriate test data\n",
    "    if 'Binary' in feature_name:\n",
    "        y_test_best = y_test_binary\n",
    "    elif 'Count' in feature_name:\n",
    "        y_test_best = y_test_count\n",
    "    elif 'TF-IDF' in feature_name:\n",
    "        y_test_best = y_test_tfidf\n",
    "    else:\n",
    "        y_test_best = y_test_w2v\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_test_best, result['predictions'])\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Ham', 'Spam'],\n",
    "                yticklabels=['Ham', 'Spam'])\n",
    "    plt.title(f'Confusion Matrix - {model_name} ({feature_name})')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix:\\n{cm}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
